{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Predict : Support phrases from given tweet text\n## Evaluation : Jaccard Score","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport plotly as py\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport re\nfrom plotly.offline import init_notebook_mode\nimport string\ninit_notebook_mode(connected = True)\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nfrom plotly.offline import iplot\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn.model_selection import KFold, GroupKFold\n\nfrom sklearn.ensemble import VotingRegressor\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n\npd.set_option('display.max_columns', None)\n#########################################################\ntrain = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-20T12:41:31.252054Z","iopub.execute_input":"2023-08-20T12:41:31.252423Z","iopub.status.idle":"2023-08-20T12:41:31.361758Z","shell.execute_reply.started":"2023-08-20T12:41:31.252392Z","shell.execute_reply":"2023-08-20T12:41:31.360757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n# First set of sentences\nActual_1 = 'My life is totally awesome'\nPredict_1 = 'awesome life'\n\n# First set of sentences\nActual_2 = 'We are active kagglers'\nPredict_2 = 'We are active kagglers'\n    \nprint(\"Jaccard score for first set of scentences: {}\".format(jaccard(Actual_1,Predict_1)))\nprint(\"Jaccard score for second set of scentences: {}\".format(jaccard(Actual_2,Predict_2)))","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:41:31.364034Z","iopub.execute_input":"2023-08-20T12:41:31.364396Z","iopub.status.idle":"2023-08-20T12:41:31.372987Z","shell.execute_reply.started":"2023-08-20T12:41:31.364362Z","shell.execute_reply":"2023-08-20T12:41:31.372002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n\nSentence_1 = 'Life well spent is life good'\nSentence_2 = 'Life is an art and it is good so far'\nSentence_3 = 'Life is good'\n\n    \nprint(jaccard(Sentence_1,Sentence_2))\nprint(jaccard(Sentence_1,Sentence_3))","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:41:31.374501Z","iopub.execute_input":"2023-08-20T12:41:31.375136Z","iopub.status.idle":"2023-08-20T12:41:31.387994Z","shell.execute_reply.started":"2023-08-20T12:41:31.375103Z","shell.execute_reply":"2023-08-20T12:41:31.386952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:41:31.390924Z","iopub.execute_input":"2023-08-20T12:41:31.391278Z","iopub.status.idle":"2023-08-20T12:41:31.406158Z","shell.execute_reply.started":"2023-08-20T12:41:31.391247Z","shell.execute_reply":"2023-08-20T12:41:31.404818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:41:31.407639Z","iopub.execute_input":"2023-08-20T12:41:31.408030Z","iopub.status.idle":"2023-08-20T12:41:31.415581Z","shell.execute_reply.started":"2023-08-20T12:41:31.407990Z","shell.execute_reply":"2023-08-20T12:41:31.414550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.dtypes\n","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:41:31.417136Z","iopub.execute_input":"2023-08-20T12:41:31.418191Z","iopub.status.idle":"2023-08-20T12:41:31.427881Z","shell.execute_reply.started":"2023-08-20T12:41:31.418156Z","shell.execute_reply":"2023-08-20T12:41:31.426861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:41:31.429490Z","iopub.execute_input":"2023-08-20T12:41:31.430734Z","iopub.status.idle":"2023-08-20T12:41:31.444490Z","shell.execute_reply.started":"2023-08-20T12:41:31.430698Z","shell.execute_reply":"2023-08-20T12:41:31.443293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:41:31.446150Z","iopub.execute_input":"2023-08-20T12:41:31.447561Z","iopub.status.idle":"2023-08-20T12:41:31.455681Z","shell.execute_reply.started":"2023-08-20T12:41:31.447461Z","shell.execute_reply":"2023-08-20T12:41:31.454586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:41:31.457209Z","iopub.execute_input":"2023-08-20T12:41:31.458680Z","iopub.status.idle":"2023-08-20T12:41:31.467578Z","shell.execute_reply.started":"2023-08-20T12:41:31.458644Z","shell.execute_reply":"2023-08-20T12:41:31.466239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## There are 12.86% of test data proportion compared to train data","metadata":{"execution":{"iopub.status.busy":"2023-08-20T11:44:13.528833Z","iopub.execute_input":"2023-08-20T11:44:13.529204Z","iopub.status.idle":"2023-08-20T11:44:13.535054Z","shell.execute_reply.started":"2023-08-20T11:44:13.529173Z","shell.execute_reply":"2023-08-20T11:44:13.534019Z"}}},{"cell_type":"markdown","source":"# Quick Data Exploration","metadata":{}},{"cell_type":"code","source":"print('Missing values in TRAIN dataset')\nfor i in train.iloc[:, 0:-1].columns.tolist():\n    print(f'{i}: {train[i].isna().sum()}')\nprint('')\nprint('Missing values in TEST dataset')\nfor i in train.iloc[:, 0:-1].columns.tolist():\n    print(f'{i}: {train[i].isna().sum()}')","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:41:31.472823Z","iopub.execute_input":"2023-08-20T12:41:31.473095Z","iopub.status.idle":"2023-08-20T12:41:31.499908Z","shell.execute_reply.started":"2023-08-20T12:41:31.473071Z","shell.execute_reply":"2023-08-20T12:41:31.498970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping missing values\ntrain.dropna(axis = 0, how ='any',inplace=True) ;","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:41:31.502347Z","iopub.execute_input":"2023-08-20T12:41:31.502660Z","iopub.status.idle":"2023-08-20T12:41:31.542928Z","shell.execute_reply.started":"2023-08-20T12:41:31.502636Z","shell.execute_reply":"2023-08-20T12:41:31.541908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Missing values in TRAIN dataset')\nfor i in train.iloc[:, 0:-1].columns.tolist():\n    print(f'{i}: {train[i].isna().sum()}')\nprint('')","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:41:31.544482Z","iopub.execute_input":"2023-08-20T12:41:31.544887Z","iopub.status.idle":"2023-08-20T12:41:31.562040Z","shell.execute_reply.started":"2023-08-20T12:41:31.544848Z","shell.execute_reply":"2023-08-20T12:41:31.561189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Positive tweet\nprint(\"Positive Tweet example :\",train[train['sentiment']=='positive']['text'].values[0])\n#negative_text\nprint(\"Negative Tweet example :\",train[train['sentiment']=='negative']['text'].values[0])\n#neutral_text\nprint(\"Neutral tweet example  :\",train[train['sentiment']=='neutral']['text'].values[0])","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:41:31.563473Z","iopub.execute_input":"2023-08-20T12:41:31.563810Z","iopub.status.idle":"2023-08-20T12:41:31.592329Z","shell.execute_reply.started":"2023-08-20T12:41:31.563780Z","shell.execute_reply":"2023-08-20T12:41:31.591356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:41:31.593418Z","iopub.execute_input":"2023-08-20T12:41:31.593682Z","iopub.status.idle":"2023-08-20T12:41:31.605402Z","shell.execute_reply.started":"2023-08-20T12:41:31.593660Z","shell.execute_reply":"2023-08-20T12:41:31.604217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['sentiment'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:41:31.606660Z","iopub.execute_input":"2023-08-20T12:41:31.607498Z","iopub.status.idle":"2023-08-20T12:41:31.620163Z","shell.execute_reply.started":"2023-08-20T12:41:31.607465Z","shell.execute_reply":"2023-08-20T12:41:31.618566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['sentiment'].value_counts(normalize=True).iplot(kind='bar',\n                                                      yTitle='Percentage', \n                                                      linecolor='black', \n                                                      opacity=0.7,\n                                                      color='red',\n                                                      theme='pearl',\n                                                      bargap=0.6,\n                                                      gridcolor='white',\n                                                     \n                                                      title='Distribution of Sentiment column in the training set')","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:41:31.621534Z","iopub.execute_input":"2023-08-20T12:41:31.622224Z","iopub.status.idle":"2023-08-20T12:41:31.675012Z","shell.execute_reply.started":"2023-08-20T12:41:31.622188Z","shell.execute_reply":"2023-08-20T12:41:31.673940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['sentiment'].value_counts(normalize=True).iplot(kind='bar',\n                                                      yTitle='Percentage', \n                                                      linecolor='black', \n                                                      opacity=0.7,\n                                                      color='red',\n                                                      theme='pearl',\n                                                      bargap=0.6,\n                                                      gridcolor='white',\n                                                      title='Distribution  of Sentiment column in the test set')","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:41:31.676585Z","iopub.execute_input":"2023-08-20T12:41:31.676951Z","iopub.status.idle":"2023-08-20T12:41:31.728634Z","shell.execute_reply.started":"2023-08-20T12:41:31.676919Z","shell.execute_reply":"2023-08-20T12:41:31.727479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Data Preprocessing","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:35:57.928390Z","iopub.execute_input":"2023-08-20T12:35:57.929568Z","iopub.status.idle":"2023-08-20T12:35:57.934727Z","shell.execute_reply.started":"2023-08-20T12:35:57.929506Z","shell.execute_reply":"2023-08-20T12:35:57.933603Z"}}},{"cell_type":"markdown","source":"Before we start with any NLP project we need to pre-process the data to get it all in a consistent format.We need to clean, tokenize and convert our data into a matrix. Let's create a function which will perform the following tasks on the text columns:\n\nMake text lowercase,\nremoves hyperlinks,\nremove punctuation\nremoves numbers\ntokenizes\nremoves stopwords","metadata":{}},{"cell_type":"code","source":"# text preprocessing helper functions\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    #remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(tokenized_text)\n    return combined_text","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:41:31.730130Z","iopub.execute_input":"2023-08-20T12:41:31.730696Z","iopub.status.idle":"2023-08-20T12:41:31.739078Z","shell.execute_reply.started":"2023-08-20T12:41:31.730664Z","shell.execute_reply":"2023-08-20T12:41:31.737971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"clean_text() function applies a first round of text cleaning techniques.the function text_preprocessing then takes in the processed text from the clean_text() function and applies techniques like tokenization and stop word removal.","metadata":{}},{"cell_type":"code","source":"# Applying the cleaning function to both test and training datasets\ntrain['text_clean'] = train['text'].apply(str).apply(lambda x: text_preprocessing(x))\ntest['text_clean'] = test['text'].apply(str).apply(lambda x: text_preprocessing(x))","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:41:31.740570Z","iopub.execute_input":"2023-08-20T12:41:31.741300Z","iopub.status.idle":"2023-08-20T12:41:32.920608Z","shell.execute_reply.started":"2023-08-20T12:41:31.741215Z","shell.execute_reply":"2023-08-20T12:41:32.919558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:41:32.922130Z","iopub.execute_input":"2023-08-20T12:41:32.922493Z","iopub.status.idle":"2023-08-20T12:41:32.936351Z","shell.execute_reply.started":"2023-08-20T12:41:32.922457Z","shell.execute_reply":"2023-08-20T12:41:32.935091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Analyzing Text Statistics","metadata":{}},{"cell_type":"markdown","source":"We can now do some statistical analysis to explore the fundamental characteristics of the text data. Some of the analysis which can be useful are:\n\nText length analysis\nword frequency analysis\nTo perform these analysis, let us create two new features i.e\n\none which calculates the length of the text, and\nsecond which calculates the word count.","metadata":{}},{"cell_type":"code","source":"train['text_len'] = train['text_clean'].astype(str).apply(len)\ntrain['text_word_count'] = train['text_clean'].apply(lambda x: len(str(x).split()))","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:42:03.951266Z","iopub.execute_input":"2023-08-20T12:42:03.951649Z","iopub.status.idle":"2023-08-20T12:42:04.021098Z","shell.execute_reply.started":"2023-08-20T12:42:03.951619Z","shell.execute_reply":"2023-08-20T12:42:04.020161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:42:20.725855Z","iopub.execute_input":"2023-08-20T12:42:20.726219Z","iopub.status.idle":"2023-08-20T12:42:20.738750Z","shell.execute_reply.started":"2023-08-20T12:42:20.726187Z","shell.execute_reply":"2023-08-20T12:42:20.737735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create three separate dataframes for positive, neutral and negative sentiments. This will help in analyzing the text statistics separately for separate polarities.","metadata":{}},{"cell_type":"code","source":"pos = train[train['sentiment']=='positive']\nneg = train[train['sentiment']=='negative']\nneutral = train[train['sentiment']=='neutral']","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:42:35.755290Z","iopub.execute_input":"2023-08-20T12:42:35.755684Z","iopub.status.idle":"2023-08-20T12:42:35.784490Z","shell.execute_reply.started":"2023-08-20T12:42:35.755652Z","shell.execute_reply":"2023-08-20T12:42:35.783573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos['text_len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='text length',\n    linecolor='black',\n    color='red',\n    yTitle='count',\n    title='Positive Text Length Distribution')\n\nneg['text_len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='text length',\n    linecolor='black',\n    color='green',\n    yTitle='count',\n    title='Negative Text Length Distribution')\n\nneutral['text_len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='text length',\n    linecolor='black',\n    yTitle='count',\n    title='Neutral Text Length Distribution')","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:42:43.897974Z","iopub.execute_input":"2023-08-20T12:42:43.898349Z","iopub.status.idle":"2023-08-20T12:42:44.401104Z","shell.execute_reply.started":"2023-08-20T12:42:43.898321Z","shell.execute_reply":"2023-08-20T12:42:44.400215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The histogram shows that the length of the cleaned text ranges from around 2 to 140 characters and generally,it is almost same for all the polarities.","metadata":{}},{"cell_type":"markdown","source":"Let's see a more consolidated comparison of the relationship of text lengths with sentiment of the text.","metadata":{}},{"cell_type":"code","source":"trace0 = go.Box(\n    y=pos['text_len'],\n    name = 'Positive Text',\n    marker = dict(\n        color = 'red',\n    )\n)\n\ntrace1 = go.Box(\n    y=neg['text_len'],\n    name = 'Negative Text',\n    marker = dict(\n        color = 'green',\n    )\n)\n\ntrace2 = go.Box(\n    y=neutral['text_len'],\n    name = 'Neutral Text',\n    marker = dict(\n        color = 'orange',\n    )\n)\ndata = [trace0, trace1, trace2]\nlayout = go.Layout(\n    title = \"Length of the text\"\n)\n\nfig = go.Figure(data=data,layout=layout)\niplot(fig, filename = \"Length of the text of different polarities\")","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:43:40.431338Z","iopub.execute_input":"2023-08-20T12:43:40.431723Z","iopub.status.idle":"2023-08-20T12:43:40.494817Z","shell.execute_reply.started":"2023-08-20T12:43:40.431693Z","shell.execute_reply":"2023-08-20T12:43:40.493911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the text appear to have more or less same length. Hence, length of the text isn't a powerful indicator of the polarity.","metadata":{}},{"cell_type":"code","source":"pos['text_word_count'].iplot(\n    kind='hist',\n    bins=50,\n    xTitle='text length',\n    linecolor='black',\n    color='red',\n    yTitle='count',\n    title='Positive Text word count')\n\nneg['text_word_count'].iplot(\n    kind='hist',\n    bins=50,\n    xTitle='text length',\n    linecolor='black',\n    color='green',\n    yTitle='count',\n    title='Negative Text word count')\nneutral['text_word_count'].iplot(\n    kind='hist',\n    bins=50,\n    xTitle='text length',\n    linecolor='black',\n    yTitle='count',\n    title='Neutral Text word count')\n","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:44:12.556898Z","iopub.execute_input":"2023-08-20T12:44:12.557269Z","iopub.status.idle":"2023-08-20T12:44:13.021172Z","shell.execute_reply.started":"2023-08-20T12:44:12.557239Z","shell.execute_reply":"2023-08-20T12:44:13.020074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, more or less, word count is also similar across positive, negative and neutral texts.This will be more clear with the Box Plots below.","metadata":{}},{"cell_type":"code","source":"trace0 = go.Box(\n    y=pos['text_word_count'],\n    name = 'Positive Text',\n    marker = dict(\n        color = 'red',\n    )\n)\n\ntrace1 = go.Box(\n    y=neg['text_word_count'],\n    name = 'Negative Text',\n    marker = dict(\n        color = 'green',\n    )\n)\n\ntrace2 = go.Box(\n    y=neutral['text_word_count'],\n    name = 'Neutral Text',\n    marker = dict(\n        color = 'orange',\n    )\n)\ndata = [trace0, trace1, trace2]\nlayout = go.Layout(\n    title = \"word count of the text\"\n)\n\nfig = go.Figure(data=data,layout=layout)\niplot(fig, filename = \"word count of the text of different polarities\")","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:44:34.233229Z","iopub.execute_input":"2023-08-20T12:44:34.233630Z","iopub.status.idle":"2023-08-20T12:44:34.283277Z","shell.execute_reply.started":"2023-08-20T12:44:34.233598Z","shell.execute_reply":"2023-08-20T12:44:34.282414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:46:40.474301Z","iopub.execute_input":"2023-08-20T12:46:40.474690Z","iopub.status.idle":"2023-08-20T12:46:40.481477Z","shell.execute_reply.started":"2023-08-20T12:46:40.474660Z","shell.execute_reply":"2023-08-20T12:46:40.480244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#source of code : https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\ndef get_top_n_words(corpus, n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:46:40.792111Z","iopub.execute_input":"2023-08-20T12:46:40.792448Z","iopub.status.idle":"2023-08-20T12:46:40.799891Z","shell.execute_reply.started":"2023-08-20T12:46:40.792420Z","shell.execute_reply":"2023-08-20T12:46:40.798872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_unigrams = get_top_n_words(pos['text_clean'],20)\nneg_unigrams = get_top_n_words(neg['text_clean'],20)\nneutral_unigrams = get_top_n_words(neutral['text_clean'],20)\n\n\n\n#for word, freq in top_unigrams:\n    #print(word, freq)\ndf1 = pd.DataFrame(pos_unigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black',color='red', title='Top 20 Unigrams in positve text',orientation='h')\n\ndf2 = pd.DataFrame(neg_unigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black', color='green',title='Top 20 Unigrams in negative text',orientation='h')\n\ndf3 = pd.DataFrame(neutral_unigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 Unigrams in neutral text',orientation='h')","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:46:41.078289Z","iopub.execute_input":"2023-08-20T12:46:41.079336Z","iopub.status.idle":"2023-08-20T12:46:42.376465Z","shell.execute_reply.started":"2023-08-20T12:46:41.079294Z","shell.execute_reply":"2023-08-20T12:46:42.375567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_top_n_gram(corpus,ngram_range,n=None):\n    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:47:03.861227Z","iopub.execute_input":"2023-08-20T12:47:03.861639Z","iopub.status.idle":"2023-08-20T12:47:03.870313Z","shell.execute_reply.started":"2023-08-20T12:47:03.861604Z","shell.execute_reply":"2023-08-20T12:47:03.869217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_bigrams = get_top_n_gram(pos['text_clean'],(2,2),20)\nneg_bigrams = get_top_n_gram(neg['text_clean'],(2,2),20)\nneutral_bigrams = get_top_n_gram(neutral['text_clean'],(2,2),20)\n\n\n\n#for word, freq in top_bigrams:\n    #print(word, freq)\ndf1 = pd.DataFrame(pos_bigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black',color='red', title='Top 20 Bigrams in positve text',orientation='h')\n\ndf2 = pd.DataFrame(neg_bigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black', color='green',title='Top 20 Bigrams in negative text',orientation='h')\n\ndf3 = pd.DataFrame(neutral_bigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 Bigrams in neutral text',orientation='h')","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:47:11.779329Z","iopub.execute_input":"2023-08-20T12:47:11.779717Z","iopub.status.idle":"2023-08-20T12:47:13.535830Z","shell.execute_reply.started":"2023-08-20T12:47:11.779688Z","shell.execute_reply":"2023-08-20T12:47:13.534709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_trigrams = get_top_n_gram(pos['text_clean'],(3,3),20)\nneg_trigrams = get_top_n_gram(neg['text_clean'],(3,3),20)\nneutral_trigrams = get_top_n_gram(neutral['text_clean'],(3,3),20)\n\ndf1 = pd.DataFrame(pos_trigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black',color='red', title='Top 20 Trigrams in positve text',orientation='h')\n\ndf2 = pd.DataFrame(neg_trigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black', color='green',title='Top 20 Trigrams in negative text',orientation='h')\n\ndf3 = pd.DataFrame(neutral_trigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 Trigrams in neutral text',orientation='h')","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:47:27.250005Z","iopub.execute_input":"2023-08-20T12:47:27.250430Z","iopub.status.idle":"2023-08-20T12:47:29.129389Z","shell.execute_reply.started":"2023-08-20T12:47:27.250396Z","shell.execute_reply":"2023-08-20T12:47:29.128450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploring the selected_text column","metadata":{}},{"cell_type":"code","source":"positive_text = train[train['sentiment'] == 'positive']['selected_text']\nnegative_text = train[train['sentiment'] == 'negative']['selected_text']\nneutral_text = train[train['sentiment'] == 'neutral']['selected_text']","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:48:05.626176Z","iopub.execute_input":"2023-08-20T12:48:05.626562Z","iopub.status.idle":"2023-08-20T12:48:05.651479Z","shell.execute_reply.started":"2023-08-20T12:48:05.626510Z","shell.execute_reply":"2023-08-20T12:48:05.650387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Positive text\nprint(\"Positive Text example :\",positive_text.values[0])\n#negative_text\nprint(\"Negative Tweet example :\",negative_text.values[0])\n#neutral_text\nprint(\"Neutral tweet example  :\",neutral_text.values[0])","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:48:10.018795Z","iopub.execute_input":"2023-08-20T12:48:10.019301Z","iopub.status.idle":"2023-08-20T12:48:10.026111Z","shell.execute_reply.started":"2023-08-20T12:48:10.019269Z","shell.execute_reply":"2023-08-20T12:48:10.025156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positive_text_clean = positive_text.apply(lambda x: text_preprocessing(x))\nnegative_text_clean = negative_text.apply(lambda x: text_preprocessing(x))\nneutral_text_clean = neutral_text.apply(lambda x: text_preprocessing(x))","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:48:16.542735Z","iopub.execute_input":"2023-08-20T12:48:16.543121Z","iopub.status.idle":"2023-08-20T12:48:17.434296Z","shell.execute_reply.started":"2023-08-20T12:48:16.543091Z","shell.execute_reply":"2023-08-20T12:48:17.433062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#source of code : https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\ndef get_top_n_words(corpus, n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:48:24.832210Z","iopub.execute_input":"2023-08-20T12:48:24.832581Z","iopub.status.idle":"2023-08-20T12:48:24.839419Z","shell.execute_reply.started":"2023-08-20T12:48:24.832545Z","shell.execute_reply":"2023-08-20T12:48:24.838198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_words_in_positive_text = get_top_n_words(positive_text_clean)\ntop_words_in_negative_text = get_top_n_words(negative_text_clean)\ntop_words_in_neutral_text = get_top_n_words(neutral_text_clean)\n\np1 = [x[0] for x in top_words_in_positive_text[:20]]\np2 = [x[1] for x in top_words_in_positive_text[:20]]\n\n\nn1 = [x[0] for x in top_words_in_negative_text[:20]]\nn2 = [x[1] for x in top_words_in_negative_text[:20]]\n\n\nnu1 = [x[0] for x in top_words_in_neutral_text[:20]]\nnu2 = [x[1] for x in top_words_in_neutral_text[:20]]","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:48:29.733410Z","iopub.execute_input":"2023-08-20T12:48:29.734525Z","iopub.status.idle":"2023-08-20T12:48:30.584182Z","shell.execute_reply.started":"2023-08-20T12:48:29.734463Z","shell.execute_reply":"2023-08-20T12:48:30.583171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\n\nfig = go.Figure([go.Bar(x=p1, y=p2, text=p2 )])\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide',title_text='Most common positive_text words')\n#fig.update_traces(texttemplate='%{text:.2s}', textposition='outside')","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:49:26.923101Z","iopub.execute_input":"2023-08-20T12:49:26.924352Z","iopub.status.idle":"2023-08-20T12:49:26.941583Z","shell.execute_reply.started":"2023-08-20T12:49:26.924319Z","shell.execute_reply":"2023-08-20T12:49:26.940709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig1 = go.Figure([go.Bar(x=n1, y=n2, text=n2,marker_color='indianred')])\n#fig1.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig1.update_layout(uniformtext_minsize=8, uniformtext_mode='hide',title_text='Most common negative_text words')","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:49:33.303336Z","iopub.execute_input":"2023-08-20T12:49:33.303736Z","iopub.status.idle":"2023-08-20T12:49:33.324601Z","shell.execute_reply.started":"2023-08-20T12:49:33.303705Z","shell.execute_reply":"2023-08-20T12:49:33.323416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig2 = go.Figure([go.Bar(x=nu1, y=nu2, text=nu2, marker_color='lightsalmon' )])\n#fig2.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig2.update_layout(uniformtext_minsize=8, uniformtext_mode='hide',title_text='Most common neutral_text words')","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:49:38.503645Z","iopub.execute_input":"2023-08-20T12:49:38.504029Z","iopub.status.idle":"2023-08-20T12:49:38.525686Z","shell.execute_reply.started":"2023-08-20T12:49:38.503997Z","shell.execute_reply":"2023-08-20T12:49:38.524557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wordclouds","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=[30, 15])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(positive_text_clean))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Positive text',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(negative_text_clean))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative text',fontsize=40);\n\nwordcloud3 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(neutral_text_clean))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Neutral text',fontsize=40);","metadata":{"execution":{"iopub.status.busy":"2023-08-20T12:50:09.144293Z","iopub.execute_input":"2023-08-20T12:50:09.144709Z","iopub.status.idle":"2023-08-20T12:50:13.282193Z","shell.execute_reply.started":"2023-08-20T12:50:09.144664Z","shell.execute_reply":"2023-08-20T12:50:13.281255Z"},"trusted":true},"execution_count":null,"outputs":[]}]}