{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:56:44.276230Z","iopub.execute_input":"2023-09-17T10:56:44.276640Z","iopub.status.idle":"2023-09-17T10:58:15.306327Z","shell.execute_reply.started":"2023-09-17T10:56:44.276603Z","shell.execute_reply":"2023-09-17T10:58:15.305349Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Loading custom CUDA kernels...\nCould not load the custom kernel for multi-scale deformable attention: /root/.cache/torch_extensions/py310_cu118/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\nXformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\npip install xformers.\n","output_type":"stream"},{"name":"stdout","text":"Using TensorFlow backend\n","output_type":"stream"},{"name":"stderr","text":"Loading custom CUDA kernels...\nUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\nCreating extension directory /root/.cache/torch_extensions/py310_cu118/cuda_kernel...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu118/cuda_kernel/build.ninja...\nBuilding extension module cuda_kernel...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n","output_type":"stream"},{"name":"stdout","text":"[1/4] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cuda_kernel -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /opt/conda/lib/python3.10/site-packages/transformers/kernels/mra/cuda_kernel.cu -o cuda_kernel.cuda.o \n[2/4] c++ -MMD -MF torch_extension.o.d -DTORCH_EXTENSION_NAME=cuda_kernel -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -c /opt/conda/lib/python3.10/site-packages/transformers/kernels/mra/torch_extension.cpp -o torch_extension.o \n[3/4] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cuda_kernel -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /opt/conda/lib/python3.10/site-packages/transformers/kernels/mra/cuda_launch.cu -o cuda_launch.cuda.o \n[4/4] c++ cuda_kernel.cuda.o cuda_launch.cuda.o torch_extension.o -shared -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cuda_kernel.so\n","output_type":"stream"},{"name":"stderr","text":"Loading extension module cuda_kernel...\n","output_type":"stream"},{"name":"stdout","text":"TF version 2.12.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders","metadata":{"execution":{"iopub.status.busy":"2023-09-17T11:00:49.659726Z","iopub.execute_input":"2023-09-17T11:00:49.660108Z","iopub.status.idle":"2023-09-17T11:00:49.665024Z","shell.execute_reply.started":"2023-09-17T11:00:49.660078Z","shell.execute_reply":"2023-09-17T11:00:49.663790Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 192\nPATH = '/kaggle/input/tf-roberta/'\ntokenizer = Tokenizer(models.BPE(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n))\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\ntrain = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv').fillna('')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T11:03:08.694352Z","iopub.execute_input":"2023-09-17T11:03:08.694711Z","iopub.status.idle":"2023-09-17T11:03:08.793954Z","shell.execute_reply.started":"2023-09-17T11:03:08.694683Z","shell.execute_reply":"2023-09-17T11:03:08.792879Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Ignored unknown kwarg option vocab_file\nIgnored unknown kwarg option merges_file\nIgnored unknown kwarg option lowercase\nIgnored unknown kwarg option add_prefix_space\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"       textID                                                         text  \\\n0  cb774db0d1                          I`d have responded, if I were going   \n1  549e992a42                Sooo SAD I will miss you here in San Diego!!!   \n2  088c60f138                                    my boss is bullying me...   \n3  9642c003ef                               what interview! leave me alone   \n4  358bd9e861   Sons of ****, why couldn`t they put them on the release...   \n\n                         selected_text sentiment  \n0  I`d have responded, if I were going   neutral  \n1                             Sooo SAD  negative  \n2                          bullying me  negative  \n3                       leave me alone  negative  \n4                        Sons of ****,  negative  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>text</th>\n      <th>selected_text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cb774db0d1</td>\n      <td>I`d have responded, if I were going</td>\n      <td>I`d have responded, if I were going</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>549e992a42</td>\n      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n      <td>Sooo SAD</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>088c60f138</td>\n      <td>my boss is bullying me...</td>\n      <td>bullying me</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9642c003ef</td>\n      <td>what interview! leave me alone</td>\n      <td>leave me alone</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>358bd9e861</td>\n      <td>Sons of ****, why couldn`t they put them on the release...</td>\n      <td>Sons of ****,</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"ct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","metadata":{"execution":{"iopub.status.busy":"2023-09-17T11:03:09.722402Z","iopub.execute_input":"2023-09-17T11:03:09.723538Z","iopub.status.idle":"2023-09-17T11:03:11.688558Z","shell.execute_reply.started":"2023-09-17T11:03:09.723493Z","shell.execute_reply":"2023-09-17T11:03:11.687449Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv').fillna('')\n\nct = test.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","metadata":{"execution":{"iopub.status.busy":"2023-09-17T11:03:11.690814Z","iopub.execute_input":"2023-09-17T11:03:11.692311Z","iopub.status.idle":"2023-09-17T11:03:11.888153Z","shell.execute_reply.started":"2023-09-17T11:03:11.692271Z","shell.execute_reply":"2023-09-17T11:03:11.887222Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-09-17T11:03:11.889349Z","iopub.execute_input":"2023-09-17T11:03:11.889692Z","iopub.status.idle":"2023-09-17T11:03:11.900791Z","shell.execute_reply.started":"2023-09-17T11:03:11.889658Z","shell.execute_reply":"2023-09-17T11:03:11.899679Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","metadata":{"execution":{"iopub.status.busy":"2023-09-17T11:03:11.902774Z","iopub.execute_input":"2023-09-17T11:03:11.903570Z","iopub.status.idle":"2023-09-17T11:03:11.917069Z","shell.execute_reply.started":"2023-09-17T11:03:11.903469Z","shell.execute_reply":"2023-09-17T11:03:11.916242Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n        \n    sv = tf.keras.callbacks.ModelCheckpoint(\n        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='auto', save_freq='epoch')\n        \n    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n        [start_tokens[idxV,], end_tokens[idxV,]]))\n    \n    print('Loading model...')\n    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    \n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]/skf.n_splits\n    preds_end += preds[1]/skf.n_splits\n    \n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b: \n            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n        else:\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-1:b])\n        all.append(jaccard(st,train.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T11:03:13.565092Z","iopub.execute_input":"2023-09-17T11:03:13.565451Z","iopub.status.idle":"2023-09-17T15:09:10.046918Z","shell.execute_reply.started":"2023-09-17T11:03:13.565420Z","shell.execute_reply":"2023-09-17T15:09:10.045950Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"loading configuration file /kaggle/input/tf-roberta/config-roberta-base.json\nModel config RobertaConfig {\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": null,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"eos_token_ids\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": null,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.33.0\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\nloading weights file /kaggle/input/tf-roberta/pretrained-roberta-base.h5\n","output_type":"stream"},{"name":"stdout","text":"#########################\n### FOLD 1\n#########################\n","output_type":"stream"},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFRobertaModel.\n\nAll the layers of TFRobertaModel were initialized from the model checkpoint at /kaggle/input/tf-roberta/pretrained-roberta-base.h5.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n687/687 [==============================] - ETA: 0s - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00\nEpoch 1: val_loss improved from inf to 0.00000, saving model to v0-roberta-0.h5\n687/687 [==============================] - 955s 1s/step - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_activation_loss: 0.0000e+00 - val_activation_1_loss: 0.0000e+00\nEpoch 2/3\n687/687 [==============================] - ETA: 0s - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00\nEpoch 2: val_loss did not improve from 0.00000\n687/687 [==============================] - 890s 1s/step - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_activation_loss: 0.0000e+00 - val_activation_1_loss: 0.0000e+00\nEpoch 3/3\n687/687 [==============================] - ETA: 0s - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00\nEpoch 3: val_loss did not improve from 0.00000\n687/687 [==============================] - 904s 1s/step - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_activation_loss: 0.0000e+00 - val_activation_1_loss: 0.0000e+00\nLoading model...\nPredicting OOF...\n172/172 [==============================] - 71s 393ms/step\nPredicting Test...\n111/111 [==============================] - 44s 392ms/step\n","output_type":"stream"},{"name":"stderr","text":"loading configuration file /kaggle/input/tf-roberta/config-roberta-base.json\nModel config RobertaConfig {\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": null,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"eos_token_ids\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": null,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.33.0\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\nloading weights file /kaggle/input/tf-roberta/pretrained-roberta-base.h5\n","output_type":"stream"},{"name":"stdout","text":">>>> FOLD 1 Jaccard = 0.0\n\n#########################\n### FOLD 2\n#########################\n","output_type":"stream"},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFRobertaModel.\n\nAll the layers of TFRobertaModel were initialized from the model checkpoint at /kaggle/input/tf-roberta/pretrained-roberta-base.h5.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n688/688 [==============================] - ETA: 0s - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00\nEpoch 1: val_loss improved from inf to 0.00000, saving model to v0-roberta-1.h5\n688/688 [==============================] - 967s 1s/step - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_activation_loss: 0.0000e+00 - val_activation_1_loss: 0.0000e+00\nEpoch 2/3\n688/688 [==============================] - ETA: 0s - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00\nEpoch 2: val_loss did not improve from 0.00000\n688/688 [==============================] - 903s 1s/step - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_activation_loss: 0.0000e+00 - val_activation_1_loss: 0.0000e+00\nEpoch 3/3\n688/688 [==============================] - ETA: 0s - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00\nEpoch 3: val_loss did not improve from 0.00000\n688/688 [==============================] - 917s 1s/step - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_activation_loss: 0.0000e+00 - val_activation_1_loss: 0.0000e+00\nLoading model...\nPredicting OOF...\n172/172 [==============================] - 71s 394ms/step\nPredicting Test...\n111/111 [==============================] - 44s 392ms/step\n","output_type":"stream"},{"name":"stderr","text":"loading configuration file /kaggle/input/tf-roberta/config-roberta-base.json\nModel config RobertaConfig {\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": null,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"eos_token_ids\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": null,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.33.0\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\nloading weights file /kaggle/input/tf-roberta/pretrained-roberta-base.h5\n","output_type":"stream"},{"name":"stdout","text":">>>> FOLD 2 Jaccard = 0.5903057955860458\n\n#########################\n### FOLD 3\n#########################\n","output_type":"stream"},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFRobertaModel.\n\nAll the layers of TFRobertaModel were initialized from the model checkpoint at /kaggle/input/tf-roberta/pretrained-roberta-base.h5.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n688/688 [==============================] - ETA: 0s - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00\nEpoch 1: val_loss improved from inf to 0.00000, saving model to v0-roberta-2.h5\n688/688 [==============================] - 966s 1s/step - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_activation_loss: 0.0000e+00 - val_activation_1_loss: 0.0000e+00\nEpoch 2/3\n688/688 [==============================] - ETA: 0s - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00\nEpoch 2: val_loss did not improve from 0.00000\n688/688 [==============================] - 917s 1s/step - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_activation_loss: 0.0000e+00 - val_activation_1_loss: 0.0000e+00\nEpoch 3/3\n688/688 [==============================] - ETA: 0s - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00\nEpoch 3: val_loss did not improve from 0.00000\n688/688 [==============================] - 903s 1s/step - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_activation_loss: 0.0000e+00 - val_activation_1_loss: 0.0000e+00\nLoading model...\nPredicting OOF...\n172/172 [==============================] - 72s 394ms/step\nPredicting Test...\n111/111 [==============================] - 44s 393ms/step\n","output_type":"stream"},{"name":"stderr","text":"loading configuration file /kaggle/input/tf-roberta/config-roberta-base.json\nModel config RobertaConfig {\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": null,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"eos_token_ids\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": null,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.33.0\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\nloading weights file /kaggle/input/tf-roberta/pretrained-roberta-base.h5\n","output_type":"stream"},{"name":"stdout","text":">>>> FOLD 3 Jaccard = 0.5910442805906191\n\n#########################\n### FOLD 4\n#########################\n","output_type":"stream"},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFRobertaModel.\n\nAll the layers of TFRobertaModel were initialized from the model checkpoint at /kaggle/input/tf-roberta/pretrained-roberta-base.h5.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n688/688 [==============================] - ETA: 0s - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00\nEpoch 1: val_loss improved from inf to 0.00000, saving model to v0-roberta-3.h5\n688/688 [==============================] - 967s 1s/step - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_activation_loss: 0.0000e+00 - val_activation_1_loss: 0.0000e+00\nEpoch 2/3\n688/688 [==============================] - ETA: 0s - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00\nEpoch 2: val_loss did not improve from 0.00000\n688/688 [==============================] - 916s 1s/step - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_activation_loss: 0.0000e+00 - val_activation_1_loss: 0.0000e+00\nEpoch 3/3\n688/688 [==============================] - ETA: 0s - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00\nEpoch 3: val_loss did not improve from 0.00000\n688/688 [==============================] - 917s 1s/step - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_activation_loss: 0.0000e+00 - val_activation_1_loss: 0.0000e+00\nLoading model...\nPredicting OOF...\n172/172 [==============================] - 71s 394ms/step\nPredicting Test...\n111/111 [==============================] - 45s 404ms/step\n","output_type":"stream"},{"name":"stderr","text":"loading configuration file /kaggle/input/tf-roberta/config-roberta-base.json\nModel config RobertaConfig {\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": null,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"eos_token_ids\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": null,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.33.0\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\nloading weights file /kaggle/input/tf-roberta/pretrained-roberta-base.h5\n","output_type":"stream"},{"name":"stdout","text":">>>> FOLD 4 Jaccard = 0.5784787933730345\n\n#########################\n### FOLD 5\n#########################\n","output_type":"stream"},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFRobertaModel.\n\nAll the layers of TFRobertaModel were initialized from the model checkpoint at /kaggle/input/tf-roberta/pretrained-roberta-base.h5.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n688/688 [==============================] - ETA: 0s - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00\nEpoch 1: val_loss improved from inf to 0.00000, saving model to v0-roberta-4.h5\n688/688 [==============================] - 950s 1s/step - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_activation_loss: 0.0000e+00 - val_activation_1_loss: 0.0000e+00\nEpoch 2/3\n688/688 [==============================] - ETA: 0s - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00\nEpoch 2: val_loss did not improve from 0.00000\n688/688 [==============================] - 916s 1s/step - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_activation_loss: 0.0000e+00 - val_activation_1_loss: 0.0000e+00\nEpoch 3/3\n688/688 [==============================] - ETA: 0s - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00\nEpoch 3: val_loss did not improve from 0.00000\n688/688 [==============================] - 914s 1s/step - loss: 0.0000e+00 - activation_loss: 0.0000e+00 - activation_1_loss: 0.0000e+00 - val_loss: 0.0000e+00 - val_activation_loss: 0.0000e+00 - val_activation_1_loss: 0.0000e+00\nLoading model...\nPredicting OOF...\n172/172 [==============================] - 71s 393ms/step\nPredicting Test...\n111/111 [==============================] - 43s 389ms/step\n>>>> FOLD 5 Jaccard = 0.0\n\n","output_type":"stream"}]},{"cell_type":"code","source":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T15:09:10.051625Z","iopub.execute_input":"2023-09-17T15:09:10.053901Z","iopub.status.idle":"2023-09-17T15:09:10.230956Z","shell.execute_reply.started":"2023-09-17T15:09:10.053862Z","shell.execute_reply":"2023-09-17T15:09:10.229939Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"test['selected_text'] = all\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)\ntest.sample(25)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T15:09:10.235431Z","iopub.execute_input":"2023-09-17T15:09:10.237791Z","iopub.status.idle":"2023-09-17T15:09:10.289559Z","shell.execute_reply.started":"2023-09-17T15:09:10.237751Z","shell.execute_reply":"2023-09-17T15:09:10.288676Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"          textID                                                         text  \\\n2307  cfd68d702e  I want it to be summer. NOW. why is it sprinkling STILL!?!?   \n1355  7c229b34c1                        _Itx no credits for me unfortunately!   \n8     e64208b4ef         and within a short time of the last clue all of them   \n352   01fb29c047  bored bored bored! wish i had something to do tomorrow, ...   \n3190  25694a4e38                           JT should just be a regular on SNL   \n2000  838c1184b4  Pissed that my bluetooth headset`s battery out lasted my...   \n3519  91f8e27d4c   Yes you are, thanks  Haha you do? Field of flowers don`...   \n2879  65019c18cb   hey Sherri -- don`t give up b/c they`re married; they m...   \n1946  4331a46aba          awhile ago it freaking hot! now it`s wet  turn off!   \n3295  08c5eb64a5                                     Sorry bro  That`s rough.   \n3108  e0c0860417                                           I love my mom tooo   \n2954  842e9e031b                         Britain`s got Torture - Edelcries...   \n2139  c0e5365bf0  School is going to be absolutely horrible today. Peace o...   \n1393  c281b14817  last class at 10:30. One final tomorrow and 2 finals on ...   \n1798  198854f289  cannot relaxing because she have to practice for her gra...   \n280   0f93ebfb23                             lol, get watching it again  hehe   \n1706  7069dd2486   I`m straining to hear but it`s very hard from all the w...   \n463   27d6472b81  $#@! My nose stud fell out and I can`t find it  Looks li...   \n2848  b2309cd1ce                                   tweeterizing from a mobile   \n1378  8862c02132  Conference call with HP.  They gave me an invalid pass c...   \n1453  5fb992345d   yes yes yes lotsa fun  i cant wait. dont like make up w...   \n3472  863e09ee74                  have fun!  i am sad to not be with you guys   \n2029  a584ebd602                                               _queen like it   \n2128  40ae4f1f0c  http://twitpic.com/4wu0a - Sunny day at vivocity, yupz j...   \n1268  b4b9d74b06   ooh, work im afarid  looking forward to a sunny weekekn...   \n\n     sentiment                                                selected_text  \n2307  negative  I want it to be summer. NOW. why is it sprinkling STILL!?!?  \n1355  negative                        _Itx no credits for me unfortunately!  \n8      neutral         and within a short time of the last clue all of them  \n352   negative  bored bored bored! wish i had something to do tomorrow, ...  \n3190   neutral                           JT should just be a regular on SNL  \n2000  negative  Pissed that my bluetooth headset`s battery out lasted my...  \n3519  positive                                                               \n2879  positive                                                               \n1946  negative          awhile ago it freaking hot! now it`s wet  turn off!  \n3295  negative                                     Sorry bro  That`s rough.  \n3108  positive                                                               \n2954  negative                         Britain`s got Torture - Edelcries...  \n2139  negative  School is going to be absolutely horrible today. Peace o...  \n1393   neutral  last class at 10:30. One final tomorrow and 2 finals on ...  \n1798   neutral  cannot relaxing because she have to practice for her gra...  \n280    neutral                             lol, get watching it again  hehe  \n1706  negative   I`m straining to hear but it`s very hard from all the w...  \n463   negative  $#@! My nose stud fell out and I can`t find it  Looks li...  \n2848   neutral                                   tweeterizing from a mobile  \n1378  negative  Conference call with HP.  They gave me an invalid pass c...  \n1453  positive                                                               \n3472   neutral                  have fun!  i am sad to not be with you guys  \n2029  positive                                                               \n2128   neutral  http://twitpic.com/4wu0a - Sunny day at vivocity, yupz j...  \n1268  negative   ooh, work im afarid  looking forward to a sunny weekekn...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>text</th>\n      <th>sentiment</th>\n      <th>selected_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2307</th>\n      <td>cfd68d702e</td>\n      <td>I want it to be summer. NOW. why is it sprinkling STILL!?!?</td>\n      <td>negative</td>\n      <td>I want it to be summer. NOW. why is it sprinkling STILL!?!?</td>\n    </tr>\n    <tr>\n      <th>1355</th>\n      <td>7c229b34c1</td>\n      <td>_Itx no credits for me unfortunately!</td>\n      <td>negative</td>\n      <td>_Itx no credits for me unfortunately!</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>e64208b4ef</td>\n      <td>and within a short time of the last clue all of them</td>\n      <td>neutral</td>\n      <td>and within a short time of the last clue all of them</td>\n    </tr>\n    <tr>\n      <th>352</th>\n      <td>01fb29c047</td>\n      <td>bored bored bored! wish i had something to do tomorrow, ...</td>\n      <td>negative</td>\n      <td>bored bored bored! wish i had something to do tomorrow, ...</td>\n    </tr>\n    <tr>\n      <th>3190</th>\n      <td>25694a4e38</td>\n      <td>JT should just be a regular on SNL</td>\n      <td>neutral</td>\n      <td>JT should just be a regular on SNL</td>\n    </tr>\n    <tr>\n      <th>2000</th>\n      <td>838c1184b4</td>\n      <td>Pissed that my bluetooth headset`s battery out lasted my...</td>\n      <td>negative</td>\n      <td>Pissed that my bluetooth headset`s battery out lasted my...</td>\n    </tr>\n    <tr>\n      <th>3519</th>\n      <td>91f8e27d4c</td>\n      <td>Yes you are, thanks  Haha you do? Field of flowers don`...</td>\n      <td>positive</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2879</th>\n      <td>65019c18cb</td>\n      <td>hey Sherri -- don`t give up b/c they`re married; they m...</td>\n      <td>positive</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1946</th>\n      <td>4331a46aba</td>\n      <td>awhile ago it freaking hot! now it`s wet  turn off!</td>\n      <td>negative</td>\n      <td>awhile ago it freaking hot! now it`s wet  turn off!</td>\n    </tr>\n    <tr>\n      <th>3295</th>\n      <td>08c5eb64a5</td>\n      <td>Sorry bro  That`s rough.</td>\n      <td>negative</td>\n      <td>Sorry bro  That`s rough.</td>\n    </tr>\n    <tr>\n      <th>3108</th>\n      <td>e0c0860417</td>\n      <td>I love my mom tooo</td>\n      <td>positive</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2954</th>\n      <td>842e9e031b</td>\n      <td>Britain`s got Torture - Edelcries...</td>\n      <td>negative</td>\n      <td>Britain`s got Torture - Edelcries...</td>\n    </tr>\n    <tr>\n      <th>2139</th>\n      <td>c0e5365bf0</td>\n      <td>School is going to be absolutely horrible today. Peace o...</td>\n      <td>negative</td>\n      <td>School is going to be absolutely horrible today. Peace o...</td>\n    </tr>\n    <tr>\n      <th>1393</th>\n      <td>c281b14817</td>\n      <td>last class at 10:30. One final tomorrow and 2 finals on ...</td>\n      <td>neutral</td>\n      <td>last class at 10:30. One final tomorrow and 2 finals on ...</td>\n    </tr>\n    <tr>\n      <th>1798</th>\n      <td>198854f289</td>\n      <td>cannot relaxing because she have to practice for her gra...</td>\n      <td>neutral</td>\n      <td>cannot relaxing because she have to practice for her gra...</td>\n    </tr>\n    <tr>\n      <th>280</th>\n      <td>0f93ebfb23</td>\n      <td>lol, get watching it again  hehe</td>\n      <td>neutral</td>\n      <td>lol, get watching it again  hehe</td>\n    </tr>\n    <tr>\n      <th>1706</th>\n      <td>7069dd2486</td>\n      <td>I`m straining to hear but it`s very hard from all the w...</td>\n      <td>negative</td>\n      <td>I`m straining to hear but it`s very hard from all the w...</td>\n    </tr>\n    <tr>\n      <th>463</th>\n      <td>27d6472b81</td>\n      <td>$#@! My nose stud fell out and I can`t find it  Looks li...</td>\n      <td>negative</td>\n      <td>$#@! My nose stud fell out and I can`t find it  Looks li...</td>\n    </tr>\n    <tr>\n      <th>2848</th>\n      <td>b2309cd1ce</td>\n      <td>tweeterizing from a mobile</td>\n      <td>neutral</td>\n      <td>tweeterizing from a mobile</td>\n    </tr>\n    <tr>\n      <th>1378</th>\n      <td>8862c02132</td>\n      <td>Conference call with HP.  They gave me an invalid pass c...</td>\n      <td>negative</td>\n      <td>Conference call with HP.  They gave me an invalid pass c...</td>\n    </tr>\n    <tr>\n      <th>1453</th>\n      <td>5fb992345d</td>\n      <td>yes yes yes lotsa fun  i cant wait. dont like make up w...</td>\n      <td>positive</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3472</th>\n      <td>863e09ee74</td>\n      <td>have fun!  i am sad to not be with you guys</td>\n      <td>neutral</td>\n      <td>have fun!  i am sad to not be with you guys</td>\n    </tr>\n    <tr>\n      <th>2029</th>\n      <td>a584ebd602</td>\n      <td>_queen like it</td>\n      <td>positive</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2128</th>\n      <td>40ae4f1f0c</td>\n      <td>http://twitpic.com/4wu0a - Sunny day at vivocity, yupz j...</td>\n      <td>neutral</td>\n      <td>http://twitpic.com/4wu0a - Sunny day at vivocity, yupz j...</td>\n    </tr>\n    <tr>\n      <th>1268</th>\n      <td>b4b9d74b06</td>\n      <td>ooh, work im afarid  looking forward to a sunny weekekn...</td>\n      <td>negative</td>\n      <td>ooh, work im afarid  looking forward to a sunny weekekn...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}